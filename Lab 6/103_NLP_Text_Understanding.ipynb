{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Natural Language Understanding\n",
    "\n",
    "In this section we go through topics related to text understanding. We cover such topics like:\n",
    "    \n",
    "- Similarity measures\n",
    "- Word Vectors\n",
    "- Vector Space Model\n",
    "- Type of vectorizers\n",
    "- Build a vectorizer with Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity measures\n",
    "\n",
    "Word does have different meanings. This makes the comparison and analysis a bit more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('developer.n.01') someone who develops real estate (especially someone who prepares a site for residential or commercial use)\n",
      "Synset('developer.n.02') photographic equipment consisting of a chemical solution for developing film\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "\n",
    "w = Word(\"developer\")\n",
    "\n",
    "for synset, definition in zip(w.get_synsets(), w.define()):\n",
    "    print(synset, definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity measures\n",
    "\n",
    "There are plenty of methods to measure the similarity of strings. Two most popular Python libraries examples for such measure are shown. We compare two strings: trains and training. The SequenceMatcher class allow us to use the Gestalt pattern matching algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "6\n",
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "a = \"training\"\n",
    "b = \"trains\"\n",
    "print(len(a))\n",
    "print(len(b))\n",
    "ratio = SequenceMatcher(None, a, b).ratio()\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance is a normalized value between 0 and 1, where 1 means identical.\n",
    "\n",
    "A different approach is shown below. We use the Jellyfish library. There are a few methods that we can use here. One of it is the Levenshtein distance. Below the distance and normalize distance values are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "import jellyfish\n",
    "distance = jellyfish.levenshtein_distance(a,b)\n",
    "print(distance)\n",
    "\n",
    "normalized_distance = distance/max(len(a),len(b))\n",
    "print(1.0-normalized_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words can be more similar to each other than other. We can build a similarity matrix to check it where 1 mean equal and 0 totally different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king king 1.0\n",
      "king queen 0.39730135\n",
      "king horse 0.41614297\n",
      "king cat 0.45909575\n",
      "king desk 0.4325951\n",
      "king lamp 0.3446691\n",
      "queen king 0.39730135\n",
      "queen queen 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen horse 0.5480645\n",
      "queen cat 0.5044127\n",
      "queen desk 0.56697273\n",
      "queen lamp 0.32840726\n",
      "horse king 0.41614297\n",
      "horse queen 0.5480645\n",
      "horse horse 1.0\n",
      "horse cat 0.6923751\n",
      "horse desk 0.5769232\n",
      "horse lamp 0.4502276\n",
      "cat king 0.45909575\n",
      "cat queen 0.5044127\n",
      "cat horse 0.6923751\n",
      "cat cat 1.0\n",
      "cat desk 0.6215685\n",
      "cat lamp 0.44732505\n",
      "desk king 0.4325951\n",
      "desk queen 0.56697273\n",
      "desk horse 0.5769232\n",
      "desk cat 0.6215685\n",
      "desk desk 1.0\n",
      "desk lamp 0.39676696\n",
      "lamp king 0.3446691\n",
      "lamp queen 0.32840726\n",
      "lamp horse 0.4502276\n",
      "lamp cat 0.44732505\n",
      "lamp desk 0.39676696\n",
      "lamp lamp 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tokens = nlp(u'king queen horse cat desk lamp')\n",
    "\n",
    "for first_token in tokens:\n",
    "    for second_token in tokens:\n",
    "        print(first_token.text, second_token.text, first_token.similarity(second_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7780197031213137\n",
      "0.6344770680568187\n",
      "0.7780197031213137\n",
      "1.0\n",
      "0.4830966075320215\n",
      "0.6344770680568187\n",
      "0.4830966075320215\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/neofelia/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u\"Warsaw is the largest city in Poland.\")\n",
    "doc2 = nlp(u\"Crossaint is baked in France.\")\n",
    "doc3 = nlp(u\"An emu is a large bird.\")\n",
    "\n",
    "for doc in [doc1, doc2, doc3]:\n",
    "    for other_doc in [doc1, doc2, doc3]:\n",
    "        print(doc.similarity(other_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity matrix looks like following:\n",
    "\n",
    "|       | doc1 | doc2 | doc3 |\n",
    "|-------|------|------|------|\n",
    "| **doc1** | 1.0  | 0.72 | 0.65 |\n",
    "| **doc 2** | 0.72 | 1.0  | 0.40 |\n",
    "| **doc 3** | 0.65 | 0.40 | 1.0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "\n",
    "SpaCy does have already a set of words that are vectorized.\n",
    "\n",
    "![](images/vectorized.png)\n",
    "\n",
    "Let's take a look at the vectors that are available in spaCy using the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king [ 6.65754     1.0223017  -2.1117556  -4.7457027  -5.0823054  -0.9653797\n",
      "  3.955317    0.18882215 -1.4628329   3.402646   -1.2039554   1.2343637\n",
      " -0.37522888 -1.0188087   2.5025377  -2.4888005   3.311638   -0.54329294\n",
      "  0.6801045   5.026564    1.9006546  -0.5771775   3.2555323   6.847481\n",
      " -1.0392618   0.99406457  1.3774668  -0.31246293  0.53067404  4.3262277\n",
      " -0.05062941  0.29697585  2.9905376  -1.9762106  -2.9259996   2.103724\n",
      "  3.399077    0.08469379 -2.0099773   1.6087633  -0.271148    0.89496684\n",
      "  0.8094448  -0.95623827 -2.1972418  -1.14282    -3.974728   -4.4042873\n",
      " -3.419536   -2.0867658  -1.2500346  -2.421174   -0.893684   -4.7800007\n",
      " -2.3787465  -2.4173367   3.9814606   0.8257359   0.05422008 -2.1352575\n",
      " -0.24285549 -2.0304685   5.1447344   0.23670977  1.5682961   1.124547\n",
      " -1.5986532   2.2118523  -3.0947537   3.9684176  -0.5337775  -3.1588178\n",
      " -0.36394206  4.9030476   2.2934537  -0.8130946  -2.073131    1.3189256\n",
      " -1.6591414  -1.4011593  -1.6979933  -2.1321511  -1.2141671   0.371443\n",
      " -4.105503    0.4202816   0.6445415  -1.8004372   2.967136   -3.5290437\n",
      " -1.6726673   2.8268912  -1.755671    3.9296212   0.17257798  4.4567747 ]\n",
      "queen [ 1.2252265  -1.7484181  -2.7208774  -2.7974038  -2.4521425   0.18316567\n",
      "  4.3347216  -1.8384229  -1.8030497   0.9934441  -2.7401628   2.7074032\n",
      "  0.9112627  -1.9744787   1.4540095   3.695192   -0.3864624   3.6875508\n",
      " -2.119126    2.4539442   0.5690623  -1.5252192   0.7122803   2.2856374\n",
      "  1.4134655   3.1263063  -0.30035365 -1.1373863   1.8207426   5.2753696\n",
      " -1.0961828   0.08482736 -0.28294563  1.2728593   2.652634    2.7559323\n",
      " -3.5527549  -0.33359784  2.5941374  -0.64916974  1.4455519   3.3759332\n",
      "  5.1066413  -1.839853   -0.9445069   1.4561722   0.5883986  -1.8181486\n",
      " -0.47925404 -1.3801303  -2.608493   -4.693453   -1.1750681  -3.695535\n",
      " -1.1855415  -2.2858696   1.4362082  -0.5066664  -2.554844    2.3341932\n",
      " -4.267321   -1.6909809   6.414498   -1.8043478   2.9968278   0.30709788\n",
      "  2.5176806   0.10910803 -0.81518507  2.0501883   0.81957227 -2.6658864\n",
      " -3.5038533  -1.7779182   2.1430786  -3.0309196  -3.2255912  -2.853363\n",
      "  1.2327933  -1.3058956  -1.6110855  -0.41460916 -2.3616033   5.169734\n",
      "  1.962064   -1.9076384  -2.3251934   1.08806     4.9964795  -1.3590367\n",
      " -0.5348892  -0.14949119  0.09666818 -0.8678773  -2.1497355   3.035077  ]\n",
      "horse [ 2.0456057  -2.879673   -0.07717046 -2.204618   -4.170737   -1.254693\n",
      "  3.878424   -2.798684    0.09666377  0.17707789 -1.7424006  -3.818424\n",
      " -0.8414619  -0.9919971   4.452192    0.3273607   1.0460911   0.59530014\n",
      " -0.486633    1.0619967   3.566668   -4.8729315   0.5597079   5.006416\n",
      "  5.365779   -0.6806142  -0.21275169 -1.9861776  -1.0400641   7.65852\n",
      " -1.2664087  -3.0270243  -0.2906884  -0.39114684  4.2235255   2.3144143\n",
      " -1.5408753   2.823039   -2.8244424  -0.632592   -2.709016    2.243716\n",
      " -0.3124076  -1.1388175  -3.2668962   0.87392074 -1.4607587  -4.0006094\n",
      "  0.37507546  0.2514519  -2.4100904  -0.82616633 -0.742314   -2.2940576\n",
      "  0.48380598 -2.190294   -1.5145477  -3.5468643   1.2626429  -1.7740376\n",
      " -4.745076   -2.114075    5.0390606  -1.2301471  -1.1498138   1.8069674\n",
      "  3.9838717   3.7846172   0.9689993   2.2088563   4.0216556  -3.3205643\n",
      "  3.2253432   0.60895634  5.5976253   0.2898975  -3.3429513  -1.0065039\n",
      " -1.1436146  -3.641761   -0.79547346  3.2754993  -1.7925063   4.420111\n",
      "  0.46356952 -0.58453614  0.02061795  0.1823624   4.7286873  -0.35833144\n",
      "  2.0021749   0.37255597 -1.1699594  -1.642492   -2.5988708   1.5518477 ]\n",
      "cat [ 4.296194   -4.2308397   0.27707696 -0.58007246 -3.3170629   1.8629777\n",
      "  3.0791345  -2.0946648  -2.058091   -0.04927468 -2.7393675  -1.0748084\n",
      "  2.3904362   0.5469217   1.5523072   0.24660361  1.987435    4.216986\n",
      " -0.81017554  2.5026336   4.134378   -3.7003207   0.03531349  5.2766767\n",
      "  2.831809    0.07754403  0.7519972  -0.8681254  -1.81948     9.485872\n",
      " -3.2947402  -0.7158364  -0.16877693 -1.4289653   6.096586   -1.0551469\n",
      " -2.3797774   3.4427342  -0.80357724  0.05011696  0.30474615  0.9814773\n",
      "  2.8381326   1.4253994  -5.8351126   1.836385    0.8558955  -2.99695\n",
      " -3.0089436  -1.7057846  -1.1163261   0.35696912 -3.550685   -1.930083\n",
      " -2.5902963  -4.048351    2.4413128  -0.09431222  0.5432124   1.2634668\n",
      " -3.9089909   0.5892216   5.46149     1.0375615  -1.5599017  -0.0303272\n",
      "  1.5248774   0.2494021  -2.4486108   1.4654634  -0.5273029  -2.3406527\n",
      "  5.4202595   3.8053722   2.6268713  -0.33991885 -1.4412649  -4.096242\n",
      " -4.687236   -3.9652877  -2.41703     1.7147365   0.15260124  1.1682948\n",
      " -1.6950433   1.809959    0.03905088 -0.16017252  2.4324749  -0.8870754\n",
      " -1.9888282   0.42500293 -0.9799231  -2.360878   -1.8707466   0.28912276]\n",
      "desk [ 3.3175693e+00 -3.2427495e+00  1.9466276e+00 -3.9451194e+00\n",
      "  6.8381172e-01 -9.4062018e-01  6.8325338e+00 -6.9991481e-01\n",
      "  8.7983626e-01  2.9354548e-01 -2.8950715e+00 -2.6993780e+00\n",
      " -1.6276686e-01 -3.0294220e+00  2.0922194e+00 -2.5769830e-01\n",
      " -5.0944880e-02  7.0746737e+00 -1.4352810e-01 -1.0489782e+00\n",
      " -2.1412592e+00 -1.8180737e+00  2.8287630e+00  5.5410147e+00\n",
      "  1.3645217e+00  4.3900937e-01 -9.3870062e-01 -2.3691146e+00\n",
      "  8.2560724e-01  9.1089458e+00 -1.9123446e+00 -5.7984877e-01\n",
      " -1.7657423e+00  1.8612996e-01  1.4009086e+00 -1.1265168e+00\n",
      " -2.3259239e+00  1.2176015e+00  8.8175511e-01  8.4201932e-01\n",
      "  6.3511676e-01  3.5056753e+00  1.9728184e+00  1.7596533e+00\n",
      " -2.8043408e+00  1.2480600e+00  6.4064622e-02 -4.7775000e-01\n",
      " -1.8285288e+00 -6.9996715e-02 -2.6419382e+00 -3.8360207e+00\n",
      " -2.6503921e+00 -3.3840435e+00 -1.2258291e-03 -1.8151796e+00\n",
      "  2.2790495e-01 -1.8749974e+00 -2.2456274e+00  1.3639323e+00\n",
      " -6.8437138e+00  9.5463562e-01  2.1625161e+00 -3.5681925e+00\n",
      " -8.1186026e-01  1.3391197e+00  8.4802794e-01  1.8150802e+00\n",
      " -2.3922944e+00  2.8025918e+00  1.9230144e+00  8.8131666e-01\n",
      "  9.3965721e-01  4.1857123e+00  2.2532520e+00 -2.8419404e+00\n",
      " -1.8210096e+00 -6.6317254e-01 -2.1322854e+00 -3.0091186e+00\n",
      "  1.4262940e+00 -3.5935807e+00  2.3478513e+00  1.6579707e+00\n",
      " -1.1645306e+00 -3.6523260e-02  2.4989800e+00 -9.1688776e-01\n",
      "  3.4314728e+00 -1.7055671e+00  4.6485308e-01 -1.0996828e+00\n",
      " -1.9469261e+00  8.1645542e-01 -1.3531921e+00  2.5160694e+00]\n",
      "lamp [ 4.1632795  -2.1989937   1.8713493   0.96235895 -0.5872425  -2.7547219\n",
      " -0.07579821 -3.296886   -2.488861    3.7877355  -2.2721994   0.42773056\n",
      " -2.3512905   1.7319536   1.2981951   0.81406844  1.3349938   2.389655\n",
      "  0.40018386  2.716382   -3.0570984  -0.0791173  -0.1671897   3.1562545\n",
      "  1.5128989   1.5015953   1.9791708   0.78980213  3.131979    0.8880419\n",
      "  0.09586847 -3.3285494  -2.3499057  -1.5543127   0.99970466  5.0118256\n",
      " -1.1016498   3.3960161  -4.8664713   1.4754846  -2.851273    6.441668\n",
      "  1.3894949   2.4629028  -1.5061302   1.6473277  -0.30255    -0.24596906\n",
      " -4.2500186   1.2067275  -2.8985572   1.3847505  -3.4491363  -2.6487029\n",
      "  0.7463585  -1.9527903  -0.5090646  -3.128681    1.5932528   0.11777997\n",
      " -4.104347   -0.82135683  7.428746   -3.6256037   0.28397128  0.8390161\n",
      " -2.7587528   0.76965725 -1.9196464   2.7673292  -1.7775333  -0.3640097\n",
      "  4.4040813   2.159934   -1.3404288   3.0232     -1.5534027  -2.6834216\n",
      " -4.609522    0.21936804 -0.38490188  1.8278329  -1.4556763   0.04736614\n",
      "  2.5481038  -3.298437    0.18710971  0.21648762  3.4787683  -1.3000877\n",
      "  1.3787888  -0.2600344  -1.8050207   1.7991118  -1.868918   -0.68955994]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tokens = nlp(u'king queen horse cat desk lamp')\n",
    "\n",
    "for token in tokens:\n",
    "    print(str(token)+\" \"+str(token.vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks that the vectors are quite long. It's easy to check the exact size of a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens[1].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around and check the vector values for some other sentences. Let's take a look at sentence vectors of one of our previous examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc1.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice example of word vectorization done by some researchers at Warsaw University: [Word2Vec](https://lamyiowce.github.io/word2viz/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling\n",
    "\n",
    "It is a simpler implementation of word2vec. It is faster as it takes only a few terms in each iteration for training insted of the whole dataset as in previous example. This is why it's called negative sampling.\n",
    "\n",
    "First of all, we define helper methods that are used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(*dims):\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def ones(*dims):\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def rand(*dims):\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "def randn(*dims):\n",
    "    return np.random.randn(*dims).astype(np.float32)\n",
    "\n",
    "def sigmoid(batch, stochastic=False):\n",
    "    return  1.0 / (1.0 + np.exp(-batch))\n",
    "\n",
    "def as_matrix(vector):\n",
    "    return np.reshape(vector, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comtrans.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2007.zip.\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/jeita.zip.\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/knbc.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/machado.zip.\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/masc_tagged.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nombank.1.0.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/propbank.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/semcor.zip.\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/universal_treebanks_v20.zip.\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/snowball_data.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping sentiment/vader_lexicon.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/neofelia/anaconda3/lib/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "nltk.download('all')\n",
    "\n",
    "from nltk.book import *\n",
    "\n",
    "texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three variables are important for the training: ``train_dict``, ``train_tokens`` and ``train_set``. The first one contain all unique words used in the corpus. The second is a list of indices of words in the dictionary that correspond to each word used in the raw text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_set = nltk.corpus.treebank_raw.raw()[0:50000].replace('.START',' ').replace(\"\\n\",\"\").replace(\".\",\" \").replace(\",\",\" \")\n",
    "#tokens = [token for token in nltk.word_tokenize(raw_set) if token.isalpha()]\n",
    "tokens = text6.tokens\n",
    "train_dict = pd.Series(tokens).unique().tolist()\n",
    "train_tokens = np.array([train_dict.index(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last variable consist of a list of two numbers. The current word index and the word index that is before the word and after the word. Depending on the window size we use also other words that are in the neighbourhood. In this example the window size is set to 2. It means we take two words before and two words after the given word and build the relation in the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "for i in range(2,len(tokens)-2):\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i-1])])\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i-2])])\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i+1])])\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i+2])])\n",
    "\n",
    "train_set = np.random.permutation(np.array(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to set the training configuration. We set the the negative samples size to 10 and the vector size to 100. Learning rate and rate decay are set to 0.1 and 0.995. The training loops are set to 8000000. Logs are displayed each 10000 epoches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = namedtuple(\"Config\", [\"dict_size\", \"vect_size\", \"neg_samples\", \"updates\", \"learning_rate\",\n",
    "                               \"learning_rate_decay\", \"decay_period\", \"log_period\"])\n",
    "conf = Config(\n",
    "    dict_size=len(train_dict),\n",
    "    vect_size=100,\n",
    "    neg_samples=10,\n",
    "    updates=8000000,\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_decay=0.995,\n",
    "    decay_period=10000,\n",
    "    log_period=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop over ``updates`` and get the word and context from the train set. We calculate the negative context and calculate the word, context and negative sample vectors. The negative context is chosen randomly. In the next step we calcualte the cost and corresponding to it gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sample(conf, train_set, train_tokens):\n",
    "    Vp = randn(conf.dict_size, conf.vect_size)\n",
    "    Vo = randn(conf.dict_size, conf.vect_size)\n",
    "\n",
    "    J = 0.0\n",
    "    learning_rate = conf.learning_rate\n",
    "    for i in range(conf.updates):\n",
    "        idx = i % len(train_set)\n",
    "\n",
    "        word = train_set[idx, 0]\n",
    "        context = train_set[idx, 1]\n",
    "\n",
    "        neg_context = np.random.randint(0, len(train_tokens), conf.neg_samples)\n",
    "        neg_context = train_tokens[neg_context]\n",
    "\n",
    "        word_vect = Vp[word, :]  # word vector\n",
    "        context_vect = Vo[context, :];  # context wector\n",
    "        negative_vects = Vo[neg_context, :]  # sampled negative vectors\n",
    "\n",
    "        # Cost and gradient calculation starts here\n",
    "        score_pos = word_vect @ context_vect.T\n",
    "        score_neg = word_vect @ negative_vects.T\n",
    "\n",
    "        J -= np.log(sigmoid(score_pos)) + np.sum(np.log(sigmoid(-score_neg)))\n",
    "        if (i + 1) % conf.log_period == 0:\n",
    "            print('Update {0}\\tcost: {1:>2.2f}'.format(i + 1, J / conf.log_period))\n",
    "            final_cost = J / conf.log_period\n",
    "            J = 0.0\n",
    "\n",
    "        pos_g = 1.0 - sigmoid(score_pos)\n",
    "        neg_g = sigmoid(score_neg)\n",
    "\n",
    "        word_grad = -pos_g * context_vect + np.sum(as_matrix(neg_g) * negative_vects, axis=0)\n",
    "        context_grad = -pos_g * word_vect\n",
    "        neg_context_grad = as_matrix(neg_g) * as_matrix(word_vect).T\n",
    "\n",
    "        Vp[word, :] -= learning_rate * word_grad\n",
    "        Vo[context, :] -= learning_rate * context_grad\n",
    "        Vo[neg_context, :] -= learning_rate * neg_context_grad\n",
    "\n",
    "        if i % conf.decay_period == 0:\n",
    "            learning_rate = learning_rate * conf.learning_rate_decay\n",
    "\n",
    "    return Vp, Vo, final_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next do the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 10000\tcost: 18.52\n",
      "Update 20000\tcost: 10.60\n",
      "Update 30000\tcost: 8.56\n",
      "Update 40000\tcost: 7.35\n",
      "Update 50000\tcost: 6.55\n",
      "Update 60000\tcost: 6.23\n",
      "Update 70000\tcost: 5.57\n",
      "Update 80000\tcost: 4.87\n",
      "Update 90000\tcost: 4.62\n",
      "Update 100000\tcost: 4.44\n",
      "Update 110000\tcost: 4.32\n",
      "Update 120000\tcost: 4.22\n",
      "Update 130000\tcost: 4.13\n",
      "Update 140000\tcost: 4.00\n",
      "Update 150000\tcost: 3.83\n",
      "Update 160000\tcost: 3.75\n",
      "Update 170000\tcost: 3.68\n",
      "Update 180000\tcost: 3.67\n",
      "Update 190000\tcost: 3.54\n",
      "Update 200000\tcost: 3.55\n",
      "Update 210000\tcost: 3.44\n",
      "Update 220000\tcost: 3.43\n",
      "Update 230000\tcost: 3.36\n",
      "Update 240000\tcost: 3.32\n",
      "Update 250000\tcost: 3.30\n",
      "Update 260000\tcost: 3.25\n",
      "Update 270000\tcost: 3.24\n",
      "Update 280000\tcost: 3.19\n",
      "Update 290000\tcost: 3.17\n",
      "Update 300000\tcost: 3.13\n",
      "Update 310000\tcost: 3.12\n",
      "Update 320000\tcost: 3.08\n",
      "Update 330000\tcost: 3.04\n",
      "Update 340000\tcost: 3.08\n",
      "Update 350000\tcost: 3.03\n",
      "Update 360000\tcost: 3.00\n",
      "Update 370000\tcost: 2.97\n",
      "Update 380000\tcost: 2.96\n",
      "Update 390000\tcost: 2.92\n",
      "Update 400000\tcost: 2.92\n",
      "Update 410000\tcost: 2.92\n",
      "Update 420000\tcost: 2.88\n",
      "Update 430000\tcost: 2.88\n",
      "Update 440000\tcost: 2.85\n",
      "Update 450000\tcost: 2.84\n",
      "Update 460000\tcost: 2.82\n",
      "Update 470000\tcost: 2.83\n",
      "Update 480000\tcost: 2.79\n",
      "Update 490000\tcost: 2.78\n",
      "Update 500000\tcost: 2.75\n",
      "Update 510000\tcost: 2.74\n",
      "Update 520000\tcost: 2.76\n",
      "Update 530000\tcost: 2.70\n",
      "Update 540000\tcost: 2.73\n",
      "Update 550000\tcost: 2.69\n",
      "Update 560000\tcost: 2.71\n",
      "Update 570000\tcost: 2.66\n",
      "Update 580000\tcost: 2.68\n",
      "Update 590000\tcost: 2.65\n",
      "Update 600000\tcost: 2.62\n",
      "Update 610000\tcost: 2.67\n",
      "Update 620000\tcost: 2.64\n",
      "Update 630000\tcost: 2.64\n",
      "Update 640000\tcost: 2.61\n",
      "Update 650000\tcost: 2.62\n",
      "Update 660000\tcost: 2.59\n",
      "Update 670000\tcost: 2.58\n",
      "Update 680000\tcost: 2.60\n",
      "Update 690000\tcost: 2.55\n",
      "Update 700000\tcost: 2.57\n",
      "Update 710000\tcost: 2.56\n",
      "Update 720000\tcost: 2.57\n",
      "Update 730000\tcost: 2.53\n",
      "Update 740000\tcost: 2.53\n",
      "Update 750000\tcost: 2.54\n",
      "Update 760000\tcost: 2.54\n",
      "Update 770000\tcost: 2.54\n",
      "Update 780000\tcost: 2.48\n",
      "Update 790000\tcost: 2.51\n",
      "Update 800000\tcost: 2.49\n",
      "Update 810000\tcost: 2.50\n",
      "Update 820000\tcost: 2.49\n",
      "Update 830000\tcost: 2.50\n",
      "Update 840000\tcost: 2.47\n",
      "Update 850000\tcost: 2.47\n",
      "Update 860000\tcost: 2.48\n",
      "Update 870000\tcost: 2.46\n",
      "Update 880000\tcost: 2.46\n",
      "Update 890000\tcost: 2.46\n",
      "Update 900000\tcost: 2.45\n",
      "Update 910000\tcost: 2.43\n",
      "Update 920000\tcost: 2.42\n",
      "Update 930000\tcost: 2.42\n",
      "Update 940000\tcost: 2.41\n",
      "Update 950000\tcost: 2.44\n",
      "Update 960000\tcost: 2.41\n",
      "Update 970000\tcost: 2.42\n",
      "Update 980000\tcost: 2.39\n",
      "Update 990000\tcost: 2.40\n",
      "Update 1000000\tcost: 2.39\n",
      "Update 1010000\tcost: 2.38\n",
      "Update 1020000\tcost: 2.43\n",
      "Update 1030000\tcost: 2.40\n",
      "Update 1040000\tcost: 2.39\n",
      "Update 1050000\tcost: 2.38\n",
      "Update 1060000\tcost: 2.38\n",
      "Update 1070000\tcost: 2.34\n",
      "Update 1080000\tcost: 2.36\n",
      "Update 1090000\tcost: 2.38\n",
      "Update 1100000\tcost: 2.36\n",
      "Update 1110000\tcost: 2.34\n",
      "Update 1120000\tcost: 2.35\n",
      "Update 1130000\tcost: 2.38\n",
      "Update 1140000\tcost: 2.32\n",
      "Update 1150000\tcost: 2.36\n",
      "Update 1160000\tcost: 2.33\n",
      "Update 1170000\tcost: 2.34\n",
      "Update 1180000\tcost: 2.35\n",
      "Update 1190000\tcost: 2.31\n",
      "Update 1200000\tcost: 2.32\n",
      "Update 1210000\tcost: 2.31\n",
      "Update 1220000\tcost: 2.32\n",
      "Update 1230000\tcost: 2.32\n",
      "Update 1240000\tcost: 2.32\n",
      "Update 1250000\tcost: 2.30\n",
      "Update 1260000\tcost: 2.30\n",
      "Update 1270000\tcost: 2.30\n",
      "Update 1280000\tcost: 2.29\n",
      "Update 1290000\tcost: 2.31\n",
      "Update 1300000\tcost: 2.30\n",
      "Update 1310000\tcost: 2.30\n",
      "Update 1320000\tcost: 2.30\n",
      "Update 1330000\tcost: 2.28\n",
      "Update 1340000\tcost: 2.27\n",
      "Update 1350000\tcost: 2.30\n",
      "Update 1360000\tcost: 2.31\n",
      "Update 1370000\tcost: 2.29\n",
      "Update 1380000\tcost: 2.29\n",
      "Update 1390000\tcost: 2.28\n",
      "Update 1400000\tcost: 2.28\n",
      "Update 1410000\tcost: 2.25\n",
      "Update 1420000\tcost: 2.26\n",
      "Update 1430000\tcost: 2.26\n",
      "Update 1440000\tcost: 2.29\n",
      "Update 1450000\tcost: 2.26\n",
      "Update 1460000\tcost: 2.23\n",
      "Update 1470000\tcost: 2.26\n",
      "Update 1480000\tcost: 2.25\n",
      "Update 1490000\tcost: 2.26\n",
      "Update 1500000\tcost: 2.24\n",
      "Update 1510000\tcost: 2.27\n",
      "Update 1520000\tcost: 2.25\n",
      "Update 1530000\tcost: 2.23\n",
      "Update 1540000\tcost: 2.22\n",
      "Update 1550000\tcost: 2.24\n",
      "Update 1560000\tcost: 2.25\n",
      "Update 1570000\tcost: 2.23\n",
      "Update 1580000\tcost: 2.25\n",
      "Update 1590000\tcost: 2.23\n",
      "Update 1600000\tcost: 2.21\n",
      "Update 1610000\tcost: 2.21\n",
      "Update 1620000\tcost: 2.23\n",
      "Update 1630000\tcost: 2.23\n",
      "Update 1640000\tcost: 2.24\n",
      "Update 1650000\tcost: 2.23\n",
      "Update 1660000\tcost: 2.22\n",
      "Update 1670000\tcost: 2.23\n",
      "Update 1680000\tcost: 2.19\n",
      "Update 1690000\tcost: 2.21\n",
      "Update 1700000\tcost: 2.20\n",
      "Update 1710000\tcost: 2.22\n",
      "Update 1720000\tcost: 2.21\n",
      "Update 1730000\tcost: 2.20\n",
      "Update 1740000\tcost: 2.20\n",
      "Update 1750000\tcost: 2.17\n",
      "Update 1760000\tcost: 2.20\n",
      "Update 1770000\tcost: 2.20\n",
      "Update 1780000\tcost: 2.22\n",
      "Update 1790000\tcost: 2.19\n",
      "Update 1800000\tcost: 2.19\n",
      "Update 1810000\tcost: 2.19\n",
      "Update 1820000\tcost: 2.17\n",
      "Update 1830000\tcost: 2.20\n",
      "Update 1840000\tcost: 2.17\n",
      "Update 1850000\tcost: 2.20\n",
      "Update 1860000\tcost: 2.19\n",
      "Update 1870000\tcost: 2.15\n",
      "Update 1880000\tcost: 2.18\n",
      "Update 1890000\tcost: 2.16\n",
      "Update 1900000\tcost: 2.20\n",
      "Update 1910000\tcost: 2.18\n",
      "Update 1920000\tcost: 2.19\n",
      "Update 1930000\tcost: 2.17\n",
      "Update 1940000\tcost: 2.19\n",
      "Update 1950000\tcost: 2.16\n",
      "Update 1960000\tcost: 2.17\n",
      "Update 1970000\tcost: 2.16\n",
      "Update 1980000\tcost: 2.17\n",
      "Update 1990000\tcost: 2.19\n",
      "Update 2000000\tcost: 2.17\n",
      "Update 2010000\tcost: 2.17\n",
      "Update 2020000\tcost: 2.15\n",
      "Update 2030000\tcost: 2.16\n",
      "Update 2040000\tcost: 2.16\n",
      "Update 2050000\tcost: 2.16\n",
      "Update 2060000\tcost: 2.15\n",
      "Update 2070000\tcost: 2.14\n",
      "Update 2080000\tcost: 2.17\n",
      "Update 2090000\tcost: 2.12\n",
      "Update 2100000\tcost: 2.15\n",
      "Update 2110000\tcost: 2.13\n",
      "Update 2120000\tcost: 2.17\n",
      "Update 2130000\tcost: 2.15\n",
      "Update 2140000\tcost: 2.13\n",
      "Update 2150000\tcost: 2.14\n",
      "Update 2160000\tcost: 2.13\n",
      "Update 2170000\tcost: 2.17\n",
      "Update 2180000\tcost: 2.14\n",
      "Update 2190000\tcost: 2.15\n",
      "Update 2200000\tcost: 2.13\n",
      "Update 2210000\tcost: 2.14\n",
      "Update 2220000\tcost: 2.14\n",
      "Update 2230000\tcost: 2.12\n",
      "Update 2240000\tcost: 2.14\n",
      "Update 2250000\tcost: 2.15\n",
      "Update 2260000\tcost: 2.15\n",
      "Update 2270000\tcost: 2.12\n",
      "Update 2280000\tcost: 2.13\n",
      "Update 2290000\tcost: 2.11\n",
      "Update 2300000\tcost: 2.12\n",
      "Update 2310000\tcost: 2.13\n",
      "Update 2320000\tcost: 2.13\n",
      "Update 2330000\tcost: 2.13\n",
      "Update 2340000\tcost: 2.10\n",
      "Update 2350000\tcost: 2.13\n",
      "Update 2360000\tcost: 2.11\n",
      "Update 2370000\tcost: 2.13\n",
      "Update 2380000\tcost: 2.11\n",
      "Update 2390000\tcost: 2.13\n",
      "Update 2400000\tcost: 2.12\n",
      "Update 2410000\tcost: 2.09\n",
      "Update 2420000\tcost: 2.13\n",
      "Update 2430000\tcost: 2.10\n",
      "Update 2440000\tcost: 2.12\n",
      "Update 2450000\tcost: 2.10\n",
      "Update 2460000\tcost: 2.14\n",
      "Update 2470000\tcost: 2.11\n",
      "Update 2480000\tcost: 2.10\n",
      "Update 2490000\tcost: 2.09\n",
      "Update 2500000\tcost: 2.10\n",
      "Update 2510000\tcost: 2.12\n",
      "Update 2520000\tcost: 2.09\n",
      "Update 2530000\tcost: 2.12\n",
      "Update 2540000\tcost: 2.11\n",
      "Update 2550000\tcost: 2.10\n",
      "Update 2560000\tcost: 2.09\n",
      "Update 2570000\tcost: 2.09\n",
      "Update 2580000\tcost: 2.09\n",
      "Update 2590000\tcost: 2.11\n",
      "Update 2600000\tcost: 2.11\n",
      "Update 2610000\tcost: 2.10\n",
      "Update 2620000\tcost: 2.11\n",
      "Update 2630000\tcost: 2.08\n",
      "Update 2640000\tcost: 2.10\n",
      "Update 2650000\tcost: 2.10\n",
      "Update 2660000\tcost: 2.11\n",
      "Update 2670000\tcost: 2.09\n",
      "Update 2680000\tcost: 2.09\n",
      "Update 2690000\tcost: 2.10\n",
      "Update 2700000\tcost: 2.06\n",
      "Update 2710000\tcost: 2.09\n",
      "Update 2720000\tcost: 2.08\n",
      "Update 2730000\tcost: 2.11\n",
      "Update 2740000\tcost: 2.09\n",
      "Update 2750000\tcost: 2.09\n",
      "Update 2760000\tcost: 2.09\n",
      "Update 2770000\tcost: 2.07\n",
      "Update 2780000\tcost: 2.09\n",
      "Update 2790000\tcost: 2.08\n",
      "Update 2800000\tcost: 2.10\n",
      "Update 2810000\tcost: 2.10\n",
      "Update 2820000\tcost: 2.07\n",
      "Update 2830000\tcost: 2.08\n",
      "Update 2840000\tcost: 2.06\n",
      "Update 2850000\tcost: 2.09\n",
      "Update 2860000\tcost: 2.08\n",
      "Update 2870000\tcost: 2.10\n",
      "Update 2880000\tcost: 2.08\n",
      "Update 2890000\tcost: 2.08\n",
      "Update 2900000\tcost: 2.05\n",
      "Update 2910000\tcost: 2.05\n",
      "Update 2920000\tcost: 2.07\n",
      "Update 2930000\tcost: 2.08\n",
      "Update 2940000\tcost: 2.08\n",
      "Update 2950000\tcost: 2.06\n",
      "Update 2960000\tcost: 2.08\n",
      "Update 2970000\tcost: 2.04\n",
      "Update 2980000\tcost: 2.07\n",
      "Update 2990000\tcost: 2.07\n",
      "Update 3000000\tcost: 2.08\n",
      "Update 3010000\tcost: 2.07\n",
      "Update 3020000\tcost: 2.05\n",
      "Update 3030000\tcost: 2.09\n",
      "Update 3040000\tcost: 2.04\n",
      "Update 3050000\tcost: 2.07\n",
      "Update 3060000\tcost: 2.05\n",
      "Update 3070000\tcost: 2.09\n",
      "Update 3080000\tcost: 2.06\n",
      "Update 3090000\tcost: 2.05\n",
      "Update 3100000\tcost: 2.06\n",
      "Update 3110000\tcost: 2.05\n",
      "Update 3120000\tcost: 2.08\n",
      "Update 3130000\tcost: 2.06\n",
      "Update 3140000\tcost: 2.08\n",
      "Update 3150000\tcost: 2.04\n",
      "Update 3160000\tcost: 2.05\n",
      "Update 3170000\tcost: 2.05\n",
      "Update 3180000\tcost: 2.05\n",
      "Update 3190000\tcost: 2.05\n",
      "Update 3200000\tcost: 2.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 3210000\tcost: 2.06\n",
      "Update 3220000\tcost: 2.04\n",
      "Update 3230000\tcost: 2.06\n",
      "Update 3240000\tcost: 2.05\n",
      "Update 3250000\tcost: 2.05\n",
      "Update 3260000\tcost: 2.05\n",
      "Update 3270000\tcost: 2.06\n",
      "Update 3280000\tcost: 2.05\n",
      "Update 3290000\tcost: 2.03\n",
      "Update 3300000\tcost: 2.07\n",
      "Update 3310000\tcost: 2.02\n",
      "Update 3320000\tcost: 2.05\n",
      "Update 3330000\tcost: 2.05\n",
      "Update 3340000\tcost: 2.06\n",
      "Update 3350000\tcost: 2.04\n",
      "Update 3360000\tcost: 2.04\n",
      "Update 3370000\tcost: 2.05\n",
      "Update 3380000\tcost: 2.03\n",
      "Update 3390000\tcost: 2.04\n",
      "Update 3400000\tcost: 2.03\n",
      "Update 3410000\tcost: 2.06\n",
      "Update 3420000\tcost: 2.04\n",
      "Update 3430000\tcost: 2.04\n",
      "Update 3440000\tcost: 2.03\n",
      "Update 3450000\tcost: 2.03\n",
      "Update 3460000\tcost: 2.06\n",
      "Update 3470000\tcost: 2.03\n",
      "Update 3480000\tcost: 2.06\n",
      "Update 3490000\tcost: 2.04\n",
      "Update 3500000\tcost: 2.04\n",
      "Update 3510000\tcost: 2.03\n",
      "Update 3520000\tcost: 2.03\n",
      "Update 3530000\tcost: 2.04\n",
      "Update 3540000\tcost: 2.03\n",
      "Update 3550000\tcost: 2.04\n",
      "Update 3560000\tcost: 2.03\n",
      "Update 3570000\tcost: 2.05\n",
      "Update 3580000\tcost: 2.02\n",
      "Update 3590000\tcost: 2.03\n",
      "Update 3600000\tcost: 2.03\n",
      "Update 3610000\tcost: 2.03\n",
      "Update 3620000\tcost: 2.03\n",
      "Update 3630000\tcost: 2.02\n",
      "Update 3640000\tcost: 2.04\n",
      "Update 3650000\tcost: 2.01\n",
      "Update 3660000\tcost: 2.04\n",
      "Update 3670000\tcost: 2.02\n",
      "Update 3680000\tcost: 2.05\n",
      "Update 3690000\tcost: 2.03\n",
      "Update 3700000\tcost: 2.02\n",
      "Update 3710000\tcost: 2.03\n",
      "Update 3720000\tcost: 2.02\n",
      "Update 3730000\tcost: 2.04\n",
      "Update 3740000\tcost: 2.02\n",
      "Update 3750000\tcost: 2.04\n",
      "Update 3760000\tcost: 2.02\n",
      "Update 3770000\tcost: 2.02\n",
      "Update 3780000\tcost: 2.02\n",
      "Update 3790000\tcost: 2.01\n",
      "Update 3800000\tcost: 2.03\n",
      "Update 3810000\tcost: 2.03\n",
      "Update 3820000\tcost: 2.04\n",
      "Update 3830000\tcost: 2.02\n",
      "Update 3840000\tcost: 2.02\n",
      "Update 3850000\tcost: 2.01\n",
      "Update 3860000\tcost: 2.00\n",
      "Update 3870000\tcost: 2.01\n",
      "Update 3880000\tcost: 2.02\n",
      "Update 3890000\tcost: 2.02\n",
      "Update 3900000\tcost: 2.00\n",
      "Update 3910000\tcost: 2.03\n",
      "Update 3920000\tcost: 2.00\n",
      "Update 3930000\tcost: 2.01\n",
      "Update 3940000\tcost: 2.02\n",
      "Update 3950000\tcost: 2.03\n",
      "Update 3960000\tcost: 2.01\n",
      "Update 3970000\tcost: 2.01\n",
      "Update 3980000\tcost: 2.03\n",
      "Update 3990000\tcost: 1.99\n",
      "Update 4000000\tcost: 2.02\n",
      "Update 4010000\tcost: 2.00\n",
      "Update 4020000\tcost: 2.04\n",
      "Update 4030000\tcost: 2.00\n",
      "Update 4040000\tcost: 2.00\n",
      "Update 4050000\tcost: 2.02\n",
      "Update 4060000\tcost: 2.01\n",
      "Update 4070000\tcost: 2.02\n",
      "Update 4080000\tcost: 2.00\n",
      "Update 4090000\tcost: 2.02\n",
      "Update 4100000\tcost: 2.01\n",
      "Update 4110000\tcost: 1.99\n",
      "Update 4120000\tcost: 1.98\n",
      "Update 4130000\tcost: 1.99\n",
      "Update 4140000\tcost: 2.00\n",
      "Update 4150000\tcost: 2.00\n",
      "Update 4160000\tcost: 2.00\n",
      "Update 4170000\tcost: 2.01\n",
      "Update 4180000\tcost: 2.02\n",
      "Update 4190000\tcost: 1.99\n",
      "Update 4200000\tcost: 2.00\n",
      "Update 4210000\tcost: 2.00\n",
      "Update 4220000\tcost: 2.01\n",
      "Update 4230000\tcost: 2.01\n",
      "Update 4240000\tcost: 1.99\n",
      "Update 4250000\tcost: 2.01\n",
      "Update 4260000\tcost: 1.98\n",
      "Update 4270000\tcost: 2.01\n",
      "Update 4280000\tcost: 1.99\n",
      "Update 4290000\tcost: 2.02\n",
      "Update 4300000\tcost: 2.00\n",
      "Update 4310000\tcost: 1.99\n",
      "Update 4320000\tcost: 2.01\n",
      "Update 4330000\tcost: 1.99\n",
      "Update 4340000\tcost: 2.00\n",
      "Update 4350000\tcost: 1.99\n",
      "Update 4360000\tcost: 2.00\n",
      "Update 4370000\tcost: 2.00\n",
      "Update 4380000\tcost: 1.99\n",
      "Update 4390000\tcost: 1.99\n",
      "Update 4400000\tcost: 1.99\n",
      "Update 4410000\tcost: 2.01\n",
      "Update 4420000\tcost: 1.99\n",
      "Update 4430000\tcost: 2.01\n",
      "Update 4440000\tcost: 1.99\n",
      "Update 4450000\tcost: 1.99\n",
      "Update 4460000\tcost: 1.99\n",
      "Update 4470000\tcost: 1.98\n",
      "Update 4480000\tcost: 2.00\n",
      "Update 4490000\tcost: 2.00\n",
      "Update 4500000\tcost: 1.99\n",
      "Update 4510000\tcost: 1.99\n",
      "Update 4520000\tcost: 1.99\n",
      "Update 4530000\tcost: 1.98\n",
      "Update 4540000\tcost: 2.00\n",
      "Update 4550000\tcost: 1.97\n",
      "Update 4560000\tcost: 2.00\n",
      "Update 4570000\tcost: 1.99\n",
      "Update 4580000\tcost: 1.98\n",
      "Update 4590000\tcost: 2.01\n",
      "Update 4600000\tcost: 1.97\n",
      "Update 4610000\tcost: 2.00\n",
      "Update 4620000\tcost: 1.98\n",
      "Update 4630000\tcost: 2.00\n",
      "Update 4640000\tcost: 1.98\n",
      "Update 4650000\tcost: 1.99\n",
      "Update 4660000\tcost: 1.99\n",
      "Update 4670000\tcost: 1.98\n",
      "Update 4680000\tcost: 1.99\n",
      "Update 4690000\tcost: 1.97\n",
      "Update 4700000\tcost: 2.00\n",
      "Update 4710000\tcost: 1.97\n",
      "Update 4720000\tcost: 1.97\n",
      "Update 4730000\tcost: 1.98\n",
      "Update 4740000\tcost: 1.97\n",
      "Update 4750000\tcost: 1.98\n",
      "Update 4760000\tcost: 1.97\n",
      "Update 4770000\tcost: 2.00\n",
      "Update 4780000\tcost: 1.97\n",
      "Update 4790000\tcost: 1.99\n",
      "Update 4800000\tcost: 1.97\n",
      "Update 4810000\tcost: 1.97\n",
      "Update 4820000\tcost: 1.98\n",
      "Update 4830000\tcost: 1.98\n",
      "Update 4840000\tcost: 1.99\n",
      "Update 4850000\tcost: 1.96\n",
      "Update 4860000\tcost: 1.98\n",
      "Update 4870000\tcost: 1.97\n",
      "Update 4880000\tcost: 1.97\n",
      "Update 4890000\tcost: 1.98\n",
      "Update 4900000\tcost: 1.99\n",
      "Update 4910000\tcost: 1.97\n",
      "Update 4920000\tcost: 1.96\n",
      "Update 4930000\tcost: 1.99\n",
      "Update 4940000\tcost: 1.97\n",
      "Update 4950000\tcost: 1.98\n",
      "Update 4960000\tcost: 1.96\n",
      "Update 4970000\tcost: 2.01\n",
      "Update 4980000\tcost: 1.97\n",
      "Update 4990000\tcost: 1.97\n",
      "Update 5000000\tcost: 1.98\n",
      "Update 5010000\tcost: 1.97\n",
      "Update 5020000\tcost: 1.99\n",
      "Update 5030000\tcost: 1.97\n",
      "Update 5040000\tcost: 2.00\n",
      "Update 5050000\tcost: 1.97\n",
      "Update 5060000\tcost: 1.97\n",
      "Update 5070000\tcost: 1.97\n",
      "Update 5080000\tcost: 1.96\n",
      "Update 5090000\tcost: 1.98\n",
      "Update 5100000\tcost: 1.97\n",
      "Update 5110000\tcost: 1.98\n",
      "Update 5120000\tcost: 1.97\n",
      "Update 5130000\tcost: 1.98\n",
      "Update 5140000\tcost: 1.96\n",
      "Update 5150000\tcost: 1.96\n",
      "Update 5160000\tcost: 1.98\n",
      "Update 5170000\tcost: 1.99\n",
      "Update 5180000\tcost: 1.97\n",
      "Update 5190000\tcost: 1.96\n",
      "Update 5200000\tcost: 1.98\n",
      "Update 5210000\tcost: 1.96\n",
      "Update 5220000\tcost: 1.97\n",
      "Update 5230000\tcost: 1.95\n",
      "Update 5240000\tcost: 1.98\n",
      "Update 5250000\tcost: 1.98\n",
      "Update 5260000\tcost: 1.95\n",
      "Update 5270000\tcost: 1.99\n",
      "Update 5280000\tcost: 1.94\n",
      "Update 5290000\tcost: 1.97\n",
      "Update 5300000\tcost: 1.95\n",
      "Update 5310000\tcost: 1.98\n",
      "Update 5320000\tcost: 1.97\n",
      "Update 5330000\tcost: 1.97\n",
      "Update 5340000\tcost: 1.96\n",
      "Update 5350000\tcost: 1.96\n",
      "Update 5360000\tcost: 1.97\n",
      "Update 5370000\tcost: 1.96\n",
      "Update 5380000\tcost: 1.99\n",
      "Update 5390000\tcost: 1.97\n",
      "Update 5400000\tcost: 1.96\n",
      "Update 5410000\tcost: 1.96\n",
      "Update 5420000\tcost: 1.96\n",
      "Update 5430000\tcost: 1.98\n",
      "Update 5440000\tcost: 1.96\n",
      "Update 5450000\tcost: 1.97\n",
      "Update 5460000\tcost: 1.96\n",
      "Update 5470000\tcost: 1.97\n",
      "Update 5480000\tcost: 1.95\n",
      "Update 5490000\tcost: 1.95\n",
      "Update 5500000\tcost: 1.97\n",
      "Update 5510000\tcost: 1.97\n",
      "Update 5520000\tcost: 1.96\n",
      "Update 5530000\tcost: 1.95\n",
      "Update 5540000\tcost: 1.97\n",
      "Update 5550000\tcost: 1.94\n",
      "Update 5560000\tcost: 1.97\n",
      "Update 5570000\tcost: 1.95\n",
      "Update 5580000\tcost: 1.97\n",
      "Update 5590000\tcost: 1.97\n",
      "Update 5600000\tcost: 1.95\n",
      "Update 5610000\tcost: 1.97\n",
      "Update 5620000\tcost: 1.96\n",
      "Update 5630000\tcost: 1.97\n",
      "Update 5640000\tcost: 1.95\n",
      "Update 5650000\tcost: 1.97\n",
      "Update 5660000\tcost: 1.94\n",
      "Update 5670000\tcost: 1.95\n",
      "Update 5680000\tcost: 1.96\n",
      "Update 5690000\tcost: 1.95\n",
      "Update 5700000\tcost: 1.96\n",
      "Update 5710000\tcost: 1.96\n",
      "Update 5720000\tcost: 1.97\n",
      "Update 5730000\tcost: 1.94\n",
      "Update 5740000\tcost: 1.96\n",
      "Update 5750000\tcost: 1.95\n",
      "Update 5760000\tcost: 1.95\n",
      "Update 5770000\tcost: 1.95\n",
      "Update 5780000\tcost: 1.96\n",
      "Update 5790000\tcost: 1.96\n",
      "Update 5800000\tcost: 1.95\n",
      "Update 5810000\tcost: 1.97\n",
      "Update 5820000\tcost: 1.94\n",
      "Update 5830000\tcost: 1.95\n",
      "Update 5840000\tcost: 1.94\n",
      "Update 5850000\tcost: 1.96\n",
      "Update 5860000\tcost: 1.95\n",
      "Update 5870000\tcost: 1.94\n",
      "Update 5880000\tcost: 1.97\n",
      "Update 5890000\tcost: 1.95\n",
      "Update 5900000\tcost: 1.96\n",
      "Update 5910000\tcost: 1.93\n",
      "Update 5920000\tcost: 1.97\n",
      "Update 5930000\tcost: 1.95\n",
      "Update 5940000\tcost: 1.95\n",
      "Update 5950000\tcost: 1.96\n",
      "Update 5960000\tcost: 1.94\n",
      "Update 5970000\tcost: 1.96\n",
      "Update 5980000\tcost: 1.94\n",
      "Update 5990000\tcost: 1.98\n",
      "Update 6000000\tcost: 1.94\n",
      "Update 6010000\tcost: 1.94\n",
      "Update 6020000\tcost: 1.95\n",
      "Update 6030000\tcost: 1.94\n",
      "Update 6040000\tcost: 1.96\n",
      "Update 6050000\tcost: 1.95\n",
      "Update 6060000\tcost: 1.95\n",
      "Update 6070000\tcost: 1.94\n",
      "Update 6080000\tcost: 1.95\n",
      "Update 6090000\tcost: 1.93\n",
      "Update 6100000\tcost: 1.94\n",
      "Update 6110000\tcost: 1.95\n",
      "Update 6120000\tcost: 1.96\n",
      "Update 6130000\tcost: 1.95\n",
      "Update 6140000\tcost: 1.93\n",
      "Update 6150000\tcost: 1.97\n",
      "Update 6160000\tcost: 1.94\n",
      "Update 6170000\tcost: 1.95\n",
      "Update 6180000\tcost: 1.94\n",
      "Update 6190000\tcost: 1.96\n",
      "Update 6200000\tcost: 1.96\n",
      "Update 6210000\tcost: 1.93\n",
      "Update 6220000\tcost: 1.95\n",
      "Update 6230000\tcost: 1.93\n",
      "Update 6240000\tcost: 1.95\n",
      "Update 6250000\tcost: 1.93\n",
      "Update 6260000\tcost: 1.96\n",
      "Update 6270000\tcost: 1.95\n",
      "Update 6280000\tcost: 1.93\n",
      "Update 6290000\tcost: 1.95\n",
      "Update 6300000\tcost: 1.93\n",
      "Update 6310000\tcost: 1.95\n",
      "Update 6320000\tcost: 1.92\n",
      "Update 6330000\tcost: 1.96\n",
      "Update 6340000\tcost: 1.94\n",
      "Update 6350000\tcost: 1.94\n",
      "Update 6360000\tcost: 1.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 6370000\tcost: 1.93\n",
      "Update 6380000\tcost: 1.95\n",
      "Update 6390000\tcost: 1.94\n",
      "Update 6400000\tcost: 1.94\n",
      "Update 6410000\tcost: 1.94\n",
      "Update 6420000\tcost: 1.96\n",
      "Update 6430000\tcost: 1.92\n",
      "Update 6440000\tcost: 1.93\n",
      "Update 6450000\tcost: 1.93\n",
      "Update 6460000\tcost: 1.96\n",
      "Update 6470000\tcost: 1.94\n",
      "Update 6480000\tcost: 1.93\n",
      "Update 6490000\tcost: 1.95\n",
      "Update 6500000\tcost: 1.93\n",
      "Update 6510000\tcost: 1.95\n",
      "Update 6520000\tcost: 1.94\n",
      "Update 6530000\tcost: 1.95\n",
      "Update 6540000\tcost: 1.94\n",
      "Update 6550000\tcost: 1.93\n",
      "Update 6560000\tcost: 1.95\n",
      "Update 6570000\tcost: 1.94\n",
      "Update 6580000\tcost: 1.95\n",
      "Update 6590000\tcost: 1.93\n",
      "Update 6600000\tcost: 1.96\n",
      "Update 6610000\tcost: 1.94\n",
      "Update 6620000\tcost: 1.93\n",
      "Update 6630000\tcost: 1.94\n",
      "Update 6640000\tcost: 1.93\n",
      "Update 6650000\tcost: 1.94\n",
      "Update 6660000\tcost: 1.93\n",
      "Update 6670000\tcost: 1.95\n",
      "Update 6680000\tcost: 1.94\n",
      "Update 6690000\tcost: 1.94\n",
      "Update 6700000\tcost: 1.92\n",
      "Update 6710000\tcost: 1.93\n",
      "Update 6720000\tcost: 1.94\n",
      "Update 6730000\tcost: 1.94\n",
      "Update 6740000\tcost: 1.95\n",
      "Update 6750000\tcost: 1.92\n",
      "Update 6760000\tcost: 1.94\n",
      "Update 6770000\tcost: 1.93\n",
      "Update 6780000\tcost: 1.94\n",
      "Update 6790000\tcost: 1.93\n",
      "Update 6800000\tcost: 1.95\n",
      "Update 6810000\tcost: 1.93\n",
      "Update 6820000\tcost: 1.92\n",
      "Update 6830000\tcost: 1.95\n",
      "Update 6840000\tcost: 1.93\n",
      "Update 6850000\tcost: 1.94\n",
      "Update 6860000\tcost: 1.92\n",
      "Update 6870000\tcost: 1.95\n",
      "Update 6880000\tcost: 1.93\n",
      "Update 6890000\tcost: 1.93\n",
      "Update 6900000\tcost: 1.93\n",
      "Update 6910000\tcost: 1.93\n",
      "Update 6920000\tcost: 1.95\n",
      "Update 6930000\tcost: 1.91\n",
      "Update 6940000\tcost: 1.94\n",
      "Update 6950000\tcost: 1.93\n",
      "Update 6960000\tcost: 1.94\n",
      "Update 6970000\tcost: 1.93\n",
      "Update 6980000\tcost: 1.93\n",
      "Update 6990000\tcost: 1.95\n",
      "Update 7000000\tcost: 1.93\n",
      "Update 7010000\tcost: 1.93\n",
      "Update 7020000\tcost: 1.94\n",
      "Update 7030000\tcost: 1.93\n",
      "Update 7040000\tcost: 1.92\n",
      "Update 7050000\tcost: 1.93\n",
      "Update 7060000\tcost: 1.93\n",
      "Update 7070000\tcost: 1.94\n",
      "Update 7080000\tcost: 1.94\n",
      "Update 7090000\tcost: 1.92\n",
      "Update 7100000\tcost: 1.93\n",
      "Update 7110000\tcost: 1.92\n",
      "Update 7120000\tcost: 1.93\n",
      "Update 7130000\tcost: 1.92\n",
      "Update 7140000\tcost: 1.94\n",
      "Update 7150000\tcost: 1.94\n",
      "Update 7160000\tcost: 1.92\n",
      "Update 7170000\tcost: 1.94\n",
      "Update 7180000\tcost: 1.92\n",
      "Update 7190000\tcost: 1.93\n",
      "Update 7200000\tcost: 1.93\n",
      "Update 7210000\tcost: 1.94\n",
      "Update 7220000\tcost: 1.92\n",
      "Update 7230000\tcost: 1.92\n",
      "Update 7240000\tcost: 1.92\n",
      "Update 7250000\tcost: 1.93\n",
      "Update 7260000\tcost: 1.95\n",
      "Update 7270000\tcost: 1.92\n",
      "Update 7280000\tcost: 1.94\n",
      "Update 7290000\tcost: 1.92\n",
      "Update 7300000\tcost: 1.92\n",
      "Update 7310000\tcost: 1.92\n",
      "Update 7320000\tcost: 1.91\n",
      "Update 7330000\tcost: 1.93\n",
      "Update 7340000\tcost: 1.93\n",
      "Update 7350000\tcost: 1.94\n",
      "Update 7360000\tcost: 1.91\n",
      "Update 7370000\tcost: 1.93\n",
      "Update 7380000\tcost: 1.91\n",
      "Update 7390000\tcost: 1.92\n",
      "Update 7400000\tcost: 1.93\n",
      "Update 7410000\tcost: 1.94\n",
      "Update 7420000\tcost: 1.93\n",
      "Update 7430000\tcost: 1.91\n",
      "Update 7440000\tcost: 1.94\n",
      "Update 7450000\tcost: 1.91\n",
      "Update 7460000\tcost: 1.94\n",
      "Update 7470000\tcost: 1.92\n",
      "Update 7480000\tcost: 1.94\n",
      "Update 7490000\tcost: 1.93\n",
      "Update 7500000\tcost: 1.92\n",
      "Update 7510000\tcost: 1.93\n",
      "Update 7520000\tcost: 1.92\n",
      "Update 7530000\tcost: 1.93\n",
      "Update 7540000\tcost: 1.91\n",
      "Update 7550000\tcost: 1.94\n",
      "Update 7560000\tcost: 1.93\n",
      "Update 7570000\tcost: 1.92\n",
      "Update 7580000\tcost: 1.92\n",
      "Update 7590000\tcost: 1.92\n",
      "Update 7600000\tcost: 1.93\n",
      "Update 7610000\tcost: 1.92\n",
      "Update 7620000\tcost: 1.93\n",
      "Update 7630000\tcost: 1.93\n",
      "Update 7640000\tcost: 1.92\n",
      "Update 7650000\tcost: 1.92\n",
      "Update 7660000\tcost: 1.92\n",
      "Update 7670000\tcost: 1.92\n",
      "Update 7680000\tcost: 1.93\n",
      "Update 7690000\tcost: 1.93\n",
      "Update 7700000\tcost: 1.91\n",
      "Update 7710000\tcost: 1.92\n",
      "Update 7720000\tcost: 1.92\n",
      "Update 7730000\tcost: 1.92\n",
      "Update 7740000\tcost: 1.92\n",
      "Update 7750000\tcost: 1.93\n",
      "Update 7760000\tcost: 1.91\n",
      "Update 7770000\tcost: 1.91\n",
      "Update 7780000\tcost: 1.94\n",
      "Update 7790000\tcost: 1.90\n",
      "Update 7800000\tcost: 1.94\n",
      "Update 7810000\tcost: 1.91\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-62145f6fa1dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-6fb9bc8928b5>\u001b[0m in \u001b[0;36mneg_sample\u001b[0;34m(conf, train_set, train_tokens)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mpos_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mneg_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Vp, Vo, J = neg_sample(conf, train_set, train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``similar_words`` can be used to find related words of the ``word``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_word_idx(word, word_dict):\n",
    "    try:\n",
    "        return np.argwhere(np.array(word_dict) == word)[0][0]\n",
    "    except:\n",
    "        raise Exception(\"No such word in dict: {}\".format(word))\n",
    "\n",
    "def similar_words(embeddings, word, word_dict, hits):\n",
    "    word_idx = lookup_word_idx(word, word_dict)\n",
    "    similarity_scores = embeddings @ embeddings[word_idx]\n",
    "    similar_word_idxs = np.argsort(-similarity_scores)    \n",
    "    return [word_dict[i] for i in similar_word_idxs[:hits]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nTraining cost: {0:>2.2f}\\n\\n'.format(J))\n",
    "\n",
    "sample_words = ['knight', 'holy', 'grail']\n",
    "\n",
    "Vp_norm = Vp / as_matrix(np.linalg.norm(Vp , axis=1))\n",
    "for w in sample_words:\n",
    "    similar = similar_words(Vp_norm, w, train_dict, 5)\n",
    "    print('Words similar to {}: {}'.format(w, \", \".join(similar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "[1] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
